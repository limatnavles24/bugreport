{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa56f48e-ebe9-424b-9224-623344d37402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch            : 1.10.1\n",
      "pytorch_lightning: 1.5.9\n",
      "torchmetrics     : 0.7.0\n",
      "tensorboard      : 2.8.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastian/miniconda3/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -p torch,pytorch_lightning,torchmetrics,tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795b2a55",
   "metadata": {},
   "source": [
    "# MLP Classifier -- Cement Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2806a0c-d9d6-446a-ac04-484c7ff934a0",
   "metadata": {},
   "source": [
    "## General settings and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7a7729c-287e-444f-810d-ba69c8af93c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 200\n",
    "LEARNING_RATE = 0.01\n",
    "NUM_WORKERS = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa08b43",
   "metadata": {},
   "source": [
    "## Setting up the PyTorch Lightning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4b5be90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955d2cf2-40c4-4ae7-b11e-f60d8bde1b7e",
   "metadata": {},
   "source": [
    "- Set up model architecture\n",
    "- Use loggers to track mean absolute errors for both the training and validation set during training; this allows us to select the best model (i.e., the epoch) based on validation set performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "148a926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(pl.LightningModule):\n",
    "    def __init__(self, input_size, hidden_units, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.train_mae = torchmetrics.MeanAbsoluteError()\n",
    "        self.valid_mae = torchmetrics.MeanAbsoluteError()\n",
    "        self.test_mae = torchmetrics.MeanAbsoluteError()\n",
    "        \n",
    "        all_layers = []\n",
    "        for hidden_unit in hidden_units:\n",
    "            layer = torch.nn.Linear(input_size, hidden_unit) \n",
    "            all_layers.append(layer) \n",
    "            all_layers.append(torch.nn.ReLU()) \n",
    "            input_size = hidden_unit\n",
    " \n",
    "        all_layers.append(torch.nn.Linear(hidden_units[-1], num_classes)) \n",
    "        all_layers.append(torch.nn.Softmax(dim=1)) \n",
    "        self.model = torch.nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = torch.nn.functional.cross_entropy(logits, y)\n",
    "        self.log(\"train_loss\", loss, on_step=True)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.train_mae.update(preds, y)\n",
    "        return loss\n",
    "\n",
    "    def training_epoch_end(self, outs):\n",
    "        self.log(\"train_mae\", self.train_mae.compute())\n",
    "        self.train_mae.reset()\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = torch.nn.functional.cross_entropy(logits, y)\n",
    "        self.log(\"valid_loss\", loss, on_step=True)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.valid_mae.update(preds, y)\n",
    "        return loss\n",
    "    \n",
    "    def validation_epoch_end(self, outs):\n",
    "        self.log(\"valid_mae\", self.valid_mae.compute())\n",
    "        self.valid_mae.reset()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = torch.nn.functional.cross_entropy(logits, y)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.test_mae.update(preds, y)\n",
    "        self.log(\"test_mae\", self.test_mae.compute())\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=LEARNING_RATE)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6aa402-3264-4949-b52b-7978e904ce96",
   "metadata": {},
   "source": [
    "## Setting up the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b417240d-dbb8-4caa-858f-139adcf97371",
   "metadata": {},
   "source": [
    "### Inspecting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61362f2d-d118-4e14-973e-b45dfc5e3b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 8\n",
      "Number of examples: 998\n",
      "Labels: [0 1 2 3 4]\n",
      "Label distribution: [196 310 244 152  96]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data_df = pd.read_csv(\"datasets/cement_strength.csv\", skiprows=1)\n",
    "data_df[\"response\"] = data_df[\"response\"]-1 # labels should start at 0\n",
    "\n",
    "data_labels = data_df[\"response\"]\n",
    "data_features = data_df.loc[:, [\"V1\", \"V2\", \"V3\", \"V4\", \"V5\", \"V6\", \"V7\", \"V8\"]]\n",
    "\n",
    "print('Number of features:', data_features.shape[1])\n",
    "print('Number of examples:', data_features.shape[0])\n",
    "print('Labels:', np.unique(data_labels.values))\n",
    "print('Label distribution:', np.bincount(data_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcf0d2d-5332-4385-beed-d712828b96e2",
   "metadata": {},
   "source": [
    "**Performance Baseline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c0bcd5f-c473-404e-9625-b597bcd9ab64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MAE: 1.03\n"
     ]
    }
   ],
   "source": [
    "avg_prediction = np.median(data_labels.values) # median minimizes MAE\n",
    "baseline_mae = np.mean(np.abs(data_labels.values - avg_prediction))\n",
    "print(f'Baseline MAE: {baseline_mae:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a6d99c-e3ee-4d32-b976-035aff3d7569",
   "metadata": {},
   "source": [
    "### Setting Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44c9a322-705b-473d-8b02-86ca99bdf06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, feature_array, label_array, dtype=np.float32):\n",
    "\n",
    "        self.features = feature_array.astype(np.float32)\n",
    "        self.labels = label_array\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        inputs = self.features[index]\n",
    "        label = self.labels[index]\n",
    "        return inputs, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b28f48b-facf-4635-b9ab-28e0844719de",
   "metadata": {},
   "source": [
    "### Setting up DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8a89532-7600-4875-86f1-2bc69810597e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_path='./'):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.transform = None\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        data_df = pd.read_csv(\n",
    "            os.path.join(self.data_path, 'cement_strength.csv'), skiprows=1)\n",
    "        data_df[\"response\"] = data_df[\"response\"]-1 # labels should start at 0\n",
    "        self.data_labels = data_df[\"response\"]\n",
    "        self.data_features = data_df.loc[:, [\n",
    "            \"V1\", \"V2\", \"V3\", \"V4\", \"V5\", \"V6\", \"V7\", \"V8\"]]\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        \n",
    "        # Split into\n",
    "        # 70% train, 10% validation, 20% testing\n",
    "        \n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "            self.data_features.values,\n",
    "            self.data_labels.values,\n",
    "            test_size=0.2,\n",
    "            random_state=1,\n",
    "            stratify=self.data_labels.values)\n",
    "\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "            X_temp,\n",
    "            y_temp,\n",
    "            test_size=0.1,\n",
    "            random_state=1,\n",
    "            stratify=y_temp)\n",
    "        \n",
    "        # Standardize features\n",
    "        sc = StandardScaler()\n",
    "        X_train_std = sc.fit_transform(X_train)\n",
    "        X_valid_std = sc.transform(X_valid)\n",
    "        X_test_std = sc.transform(X_test)\n",
    "        \n",
    "        self.train = MyDataset(X_train_std, y_train)\n",
    "        self.valid = MyDataset(X_valid_std, y_valid) \n",
    "        self.test = MyDataset(X_test_std, y_test)\n",
    "        \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, batch_size=64, num_workers=NUM_WORKERS)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.valid, batch_size=64, num_workers=NUM_WORKERS)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test, batch_size=64, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    \n",
    "torch.manual_seed(1) \n",
    "data_module = DataModule(data_path='datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24cb15b1-d2ef-4d2b-940c-5006e08ee0f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "718"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module.prepare_data()\n",
    "data_module.setup()\n",
    "len(data_module.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6f8d80-236a-4b01-9cbe-77eb300a0792",
   "metadata": {},
   "source": [
    "### Training the model using the PyTorch Lightning Trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36f5eeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/Users/sebastian/miniconda3/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:469: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  rank_zero_deprecation(\n",
      "\n",
      "  | Name      | Type              | Params\n",
      "------------------------------------------------\n",
      "0 | train_mae | MeanAbsoluteError | 0     \n",
      "1 | valid_mae | MeanAbsoluteError | 0     \n",
      "2 | test_mae  | MeanAbsoluteError | 0     \n",
      "3 | model     | Sequential        | 901   \n",
      "------------------------------------------------\n",
      "901       Trainable params\n",
      "0         Non-trainable params\n",
      "901       Total params\n",
      "0.004     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastian/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/sebastian/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/sebastian/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:432: UserWarning: The number of training samples (12) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  86%|██████████▎ | 12/14 [00:00<00:00, 396.27it/s, loss=1.58, v_num=14]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|████████████| 14/14 [00:00<00:00, 353.40it/s, loss=1.58, v_num=14]\u001b[A\n",
      "Epoch 1:  86%|██████████▎ | 12/14 [00:00<00:00, 473.43it/s, loss=1.54, v_num=14]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|████████████| 14/14 [00:00<00:00, 416.62it/s, loss=1.54, v_num=14]\u001b[A\n",
      "Epoch 2:  86%|██████████▎ | 12/14 [00:00<00:00, 514.91it/s, loss=1.47, v_num=14]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|████████████| 14/14 [00:00<00:00, 449.45it/s, loss=1.47, v_num=14]\u001b[A\n",
      "Epoch 3:  86%|██████████▎ | 12/14 [00:00<00:00, 520.97it/s, loss=1.42, v_num=14]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|████████████| 14/14 [00:00<00:00, 445.93it/s, loss=1.42, v_num=14]\u001b[A\n",
      "Epoch 4:  86%|██████████▎ | 12/14 [00:00<00:00, 456.83it/s, loss=1.37, v_num=14]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|████████████| 14/14 [00:00<00:00, 408.06it/s, loss=1.37, v_num=14]\u001b[A\n",
      "Epoch 5:  86%|██████████▎ | 12/14 [00:00<00:00, 528.38it/s, loss=1.33, v_num=14]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|████████████| 14/14 [00:00<00:00, 463.25it/s, loss=1.33, v_num=14]\u001b[A\n",
      "Epoch 6:  86%|███████████▏ | 12/14 [00:00<00:00, 485.38it/s, loss=1.3, v_num=14]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█████████████| 14/14 [00:00<00:00, 428.54it/s, loss=1.3, v_num=14]\u001b[A\n",
      "Epoch 7:  86%|██████████▎ | 12/14 [00:00<00:00, 612.12it/s, loss=1.28, v_num=14]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|████████████| 14/14 [00:00<00:00, 530.44it/s, loss=1.28, v_num=14]\u001b[A\n",
      "Epoch 8:  86%|██████████▎ | 12/14 [00:00<00:00, 618.23it/s, loss=1.26, v_num=14]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|████████████| 14/14 [00:00<00:00, 511.67it/s, loss=1.26, v_num=14]\u001b[A\n",
      "Epoch 9:  86%|██████████▎ | 12/14 [00:00<00:00, 639.86it/s, loss=1.24, v_num=14]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|████████████| 14/14 [00:00<00:00, 545.70it/s, loss=1.24, v_num=14]\u001b[A\n",
      "Epoch 9: 100%|████████████| 14/14 [00:00<00:00, 480.70it/s, loss=1.24, v_num=14]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "\n",
    "model = MultiLayerPerceptron(\n",
    "    input_size=data_features.shape[1],\n",
    "    hidden_units=(32, 16),\n",
    "    num_classes=np.bincount(data_labels).shape[0])\n",
    "\n",
    "\n",
    "callbacks = [ModelCheckpoint(save_top_k=1, mode='max', monitor=\"valid_mae\")] # save top 1 model\n",
    "\n",
    "if torch.cuda.is_available(): # if you have GPUs\n",
    "    gpus = 1 # number of GPUs to use\n",
    "else:\n",
    "    gpus = None\n",
    "\n",
    "    \n",
    "logger = CSVLogger(save_dir=\"logs/\", name=\"mlp-crossentropy-cement\", flush_logs_every_n_steps=1)\n",
    "    \n",
    "trainer = pl.Trainer(max_epochs=10, callbacks=callbacks, gpus=gpus, logger=logger)\n",
    "trainer.fit(model=model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a15f2cf-bb14-4ab0-b2bb-9fdeaff6430b",
   "metadata": {},
   "source": [
    "### Evaluating the model using TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "596ae7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs/mlp-crossentropy-cement/version_14/checkpoints/epoch=0-step=11.ckpt\n",
      "Loaded model weights from checkpoint at logs/mlp-crossentropy-cement/version_14/checkpoints/epoch=0-step=11.ckpt\n",
      "/Users/sebastian/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0it [00:00, ?it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_loss': 1.5538876056671143, 'test_mae': 0.9420334100723267}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|███████████████████████████████████| 4/4 [00:00<00:00, 854.67it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 1.5538876056671143, 'test_mae': 0.9420334100723267}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model=model, datamodule=data_module, ckpt_path='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70424b9f-e2b1-47fd-8775-83f13cffe52f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21561f95-aec4-4475-8cde-76de5b09582f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
